---
title: "This American R"
author: "Chris Cote"
date: '2020-02-14'
slug: this-american-r
categories: Text
---

## Motivation
I love This American Life. I thought it would be a fun corpus to look at and do some text analysis on. I'm still working on getting a public and updated version of this dataset up, but for now, let's just look at what we can see!

## Setup
Load tidyverse, set theme, read in local file...
```{r setup, message = FALSE, warning = FALSE, echo = TRUE}
library(tidyverse)
```

```{r}
theme_cc <- theme(panel.grid = element_line(color = "gray90"),
        panel.background = element_blank(),
        plot.background = element_blank(),
        plot.title = element_text(hjust = 0.5))
```

```{r, cache = TRUE}
tal <- read_csv("~/Documents/HKS/rtutorial/data/this_american_life/tal_complete.csv")

```

## Cleaning
Do some preliminary cleaning, taking out carriage return characters, getting rid of stopwords -- all on first 100 episodes only
```{r, message = FALSE, warning = FALSE, echo = TRUE}
tal_tidy <- tal %>%  slice(1:100) %>% 
  tidytext::unnest_tokens("word", content) %>% 
  mutate(title = str_replace_all(title, "\\\n", " ") %>% 
           str_replace(., "Transcript.+Originally aired.+", "")) %>% 
  anti_join(tidytext::stop_words)
```


Let's create a file that gives us the tf_idf of the whole corpus. 

Read more in the fantastic [Tidy Text Mining] book (https://www.tidytextmining.com/tfidf.html)

[(oops! ... maybe there is a better way to do this, thanks Julia Silge)](https://juliasilge.com/blog/introducing-tidylo/)
```{r, message = FALSE, warning = FALSE, echo = TRUE}
tal_tf_idf <- tal_tidy  %>% 
  select(title, word) %>% 
  group_by(title) %>% 
  count(word, sort = TRUE) %>% 
  ungroup() %>% 
  tidytext::bind_tf_idf(word, title, n)
  
```

Using the `tf_idf dataset`, let's look at the most important words in the first 50 episodes ever. 
```{r fig.height=20, fig.width = 6.5, message = FALSE, warning = FALSE, echo = TRUE}
tal_tf_idf %>% 
  group_by(title) %>% 
  arrange(-tf_idf) %>% 
  slice(1:10) %>% 
  summarise(top_words = str_c(word, collapse = ", ")) %>% 
  ungroup() %>% 
  left_join(tal_tidy %>% 
  distinct(title, air_date, show_number)) %>% 
  arrange(air_date) %>% 
  mutate(title_words = str_c(title, top_words, sep = ": ")) %>% 
  ggplot() + 
  geom_text(aes(-show_number, -5, label = title_words), size = 3, hjust = 0) + 
  # scale_x_date(date_labels = "%y-%m-%d") +
  theme(axis.text = element_blank(),
        axis.ticks = element_blank(),
        panel.background = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
    scale_y_continuous(limits = c(-6, 6)) +
  coord_flip() + 
  labs(x = NULL, y = NULL, title = "Episode Title and Unique Words") +
  NULL
```


OK now let's look at the words that are most unique per episode and most frequently occuring overall

Hmmm... Bob Dole and Frank Sinatra! Ira has some questions to answer.
```{r, message = FALSE, warning = FALSE, echo = TRUE}
tt <- tal_tf_idf %>% 
  arrange(-tf_idf) %>% 
  group_by(word) %>% 
  mutate(total_n = sum(n)) %>% 
  ungroup()

tt %>% 
  group_by(word) %>% 
  mutate(total_shows = n()) %>% 
  ungroup() %>% 
  filter(tf_idf > 0.035,
         total_shows > 1) %>% 
  ggplot(aes(tf_idf, total_n)) + 
  geom_point(aes(size = total_shows, alpha = tf_idf), shape = 21) +
  ggrepel::geom_text_repel(aes(label = word), size = 3) +
  scale_y_log10() +
  theme_cc

```




### How frequently is David Sedaris on TAL?


![](/post/2020-02-14-this-american-r-blog_files/sedaris.png)

Frequent listeners know that David Sedaris is a frequent guest on This American Life. The question is: how frequent? Sometimes it feels like he's on every 10 episodes. Is this true?



The chart below shows time between each episode that features David Sedaris.


```{r, message = FALSE, warning = FALSE, echo = TRUE}
tal %>% filter(str_detect(content, regex("sedaris",ignore_case = TRUE))) %>%   tidytext::unnest_tokens("word", content) %>% 
  filter(word == "sedaris") %>% 
  group_by(show_number, air_date) %>% 
  count() %>% 
  ungroup() %>% 
  arrange(air_date) %>% 
  mutate(time_between = as.numeric(air_date - lag(air_date))) %>% 
  ggplot(aes(show_number, time_between)) +
  geom_point() +
  geom_line() +
  theme_cc +
  labs(title = "Where's David?", x = "Show Number", y = "Days since Last Sedaris Appearance")
```


```{r, fig.align = "center", fig.width= 10, fig.height = 8}
tal %>% filter(str_detect(content, regex("sedaris",ignore_case = TRUE))) %>%    
  tidytext::unnest_tokens("word", content) %>% 
  filter(show_number < 50) %>% 
  mutate(title = str_replace_all(title, "\\\n", " ") %>% 
           str_replace(., "Transcript.+Originally aired.+", "")) %>% 
  anti_join(tidytext::stop_words) %>% 
  select(title, word) %>% 
  group_by(title) %>% 
  count(word, sort = TRUE) %>% 
  ungroup() %>% 
  tidytext::bind_tf_idf(word, title, n) %>% 
  group_by(title) %>% 
  top_n(10, tf_idf) %>% 
  ungroup() %>% 
  mutate(title = str_wrap(title, 30)) %>% 
  ggplot() + geom_col(aes(fct_reorder(word, tf_idf), tf_idf,  fill = word), show.legend = FALSE) +
  facet_wrap(~title, scales = "free_y") +
  coord_flip() + 
  theme(legend.position = "none") +
  theme_cc + 
  labs(x = NULL,y = NULL, title = "Top words in Episodes featuring David Sedaris") +
  theme(strip.background = element_blank())
  
```


